{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10316d5779a3733",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Exercise 2: Boosting\n",
    "\n",
    "## Do not start the exercise until you fully understand the submission guidelines.\n",
    "\n",
    "\n",
    "* The homework assignments are executed automatically. \n",
    "* Failure to comply with the following instructions will result in a significant penalty. \n",
    "* Appeals regarding your failure to read these instructions will be denied. \n",
    "* Kind reminder: the homework assignments contribute 60% of the final grade.\n",
    "\n",
    "\n",
    "## Read the following instructions carefully:\n",
    "\n",
    "1. This Jupyter notebook contains all the step-by-step instructions needed for this exercise.\n",
    "1. Write **efficient**, **vectorized** code whenever possible. Some calculations in this exercise may take several minutes when implemented efficiently, and might take much longer otherwise. Unnecessary loops will result in point deductions.\n",
    "1. You are responsible for the correctness of your code and should add as many tests as you see fit to this jupyter notebook. Tests will not be graded nor checked.\n",
    "1. You are allowed to use functions and methods from the [Python Standard Library](https://docs.python.org/3/library/).\n",
    "1. Your code must run without errors. Use at least `numpy` 1.15.4. Any code that cannot run will not be graded.\n",
    "1. Write your own code. Cheating will not be tolerated.\n",
    "1. Submission includes a zip file that contains this notebook, with your ID as the file name. For example, `hw1_123456789_987654321.zip` if you submitted in pairs and `hw1_123456789.zip` if you submitted the exercise alone. The name of the notebook should follow the same structure.\n",
    "   \n",
    "Please use only a **zip** file in your submission.\n",
    "\n",
    "---\n",
    "---\n",
    "\n",
    "## Please sign that you have read and understood the instructions: \n",
    "\n",
    "### *** YOUR RUNI EMAILS HERE ***\n",
    "---\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "735832cbfa43f83",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "from sklearn.datasets import make_classification, make_gaussian_quantiles, make_moons\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier, RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.decomposition import PCA\n",
    "from itertools import product\n",
    "import pandas as pd\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f877cca-f7d1-41ca-9d3d-28d587bec85c",
   "metadata": {},
   "source": [
    "# Design your algorithm\n",
    "Make sure to describe the algorithm, its limitations, and describe use-cases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ba7760b-b4ef-45a9-9aad-88b45fd8d442",
   "metadata": {},
   "source": [
    "Our design begins by initializing every sample with equal weight. In each of the T rounds, we train a simple decision stump on the weighted data, compute its weighted error $\\epsilon$, and assign it a weight:\n",
    "$w_i = \\frac{w_i \\cdot e^{-\\alpha y_i h(x_i)}}{2\\sqrt{\\epsilon\\cdot\\left(1-\\epsilon\\right)}}$, where $\\alpha \\;=\\;\\tfrac{1}{2}\\ln\\!\\bigl(\\tfrac{1-\\epsilon}{\\epsilon}\\bigr)$.\n",
    "We boost the weights of misclassified points and lower those of correctly classified ones. After T iterations, the final classifier is\n",
    "$H(x)\\;=\\;\\mathrm{sign}\\Bigl(\\sum_{t=1}^T\\alpha_t\\,h_t(x)\\Bigr)$,\n",
    "a weighted majority vote over all stumps.\n",
    "\n",
    "Key points of the design include the number of boosting rounds T, the learning rate (which scales each $\\alpha$), and a stability mechanism (such as adding a minimum $\\epsilon$ to avoid $\\ln(0)$ and early stopping when $\\epsilon \\ge 0.5$). A smaller learning rate with more rounds yields smoother convergence, while a larger rate learns faster but can overfit noisy data. Balancing these settings controls accuracy, robustness to noise, and computational efficiency.\n",
    "\n",
    "Examples for limitations of the algorithm are overfitting noisy or mislabeled data and slow running time when too many rounds (T) are used. The algorithm will work well on clean, structured data where weak learners can slowly improve the decision line. Also, it will work well for classification problems with a small number of features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2158ff2629daf2bb",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Your implementations\n",
    "You may add new cells, write helper functions or test code as you see fit.\n",
    "Please use the cell below and include a description of your implementation.\n",
    "Explain code design consideration, algorithmic choices and any other details you think is relevant to understanding your implementation.\n",
    "Failing to explain your code will lead to point deductions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe206c9-d1f4-440b-aca0-c807cdd79451",
   "metadata": {},
   "source": [
    " In this implementation of t-SNE, we have followed the core principles of the algorithm as thought in class.\n",
    " \n",
    " As described in the design we start by giving equal weight to all n samples ($w_i = \\tfrac{1}{n}$), then for each of up to T rounds we train a decision stump (DecisionTreeClassifier(max_depth=1)) using those weights. After predicting on the training set, we compute the weighted error:\n",
    "$\\epsilon = \\sum_{h_t(x_i) \\neq y_i} w_i$.\n",
    "\n",
    "As explained in the design, we update each sample’s weight by multiplying it with $\\exp(-\\alpha\\,y_i\\,h(x_i))$ which increases weights for misclassified points and decreases weights for correctly classified points. Then, we normalize with $2\\sqrt{\\epsilon(1-\\epsilon)}$ to keep the weight vector summing to one. Early stopping when $\\epsilon \\ge 0.5$ prevents adding a stump that’s no better than random.\n",
    "\n",
    "Prediction simply aggregates each stump’s vote weighted by its $\\alpha$:\n",
    "$H(x)=\\mathrm{sign}\\Bigl(\\sum_t \\alpha_t\\,h_t(x)\\Bigr)$.\n",
    "\n",
    "Our implementation includes a predict_proba method that turns the weighted sum of stumps into probabilities using a simple logistic sigmoid, so we can later plot ROC curves and compute AUC. The main hyperparameters we have are the number of rounds (T) and learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b85d8f7447ebce0",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Your code here\n",
    "# Part 1: Implementing AdaBoost\n",
    "class AdaBoostCustom:\n",
    "    def __init__(self, T=10, epsilon=1e-10, learning_rate=1.0):\n",
    "        self.T = T\n",
    "        self.alphas = []\n",
    "        self.models = []\n",
    "        # Note: You may add more attributes\n",
    "        self.epsilon = epsilon    \n",
    "        self.learning_rate = learning_rate  \n",
    "\n",
    "    def fit(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "\n",
    "        # init weights\n",
    "        w = np.ones(n_samples) / n_samples\n",
    "\n",
    "        for t in range(self.T):\n",
    "            # using a weak classifier (decision stump)\n",
    "            model = DecisionTreeClassifier(max_depth=1)\n",
    "            model.fit(X, y, sample_weight=w)\n",
    "            y_pred = model.predict(X)\n",
    "\n",
    "            # compute the error\n",
    "            err = np.sum(w * (y_pred != y)) / np.sum(w)\n",
    "\n",
    "            if err >= 0.5:\n",
    "                break\n",
    "\n",
    "            # compute alpha\n",
    "            alpha = self.learning_rate * 0.5 * np.log((1 - err) / (err + self.epsilon))\n",
    "\n",
    "            self.alphas.append(alpha)\n",
    "            self.models.append(model)\n",
    "\n",
    "            # update weights\n",
    "            # (increase weights for misclassified samples\n",
    "            # decrease weights for correctly classified samples)\n",
    "            w *= np.exp(-alpha * y * y_pred)\n",
    "            w /= 2 * np.sqrt(err * (1 - err) + self.epsilon)\n",
    "    \n",
    "    def predict(self, X):  \n",
    "        y_pred = np.zeros(X.shape[0])\n",
    "\n",
    "        # get predictions from all models\n",
    "        for alpha, model in zip(self.alphas, self.models):\n",
    "            y_pred += alpha * model.predict(X)\n",
    "\n",
    "        # return the sign of the accumulated predictions\n",
    "        return np.sign(y_pred)\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        # approximate probabilities via sigmoid of margin\n",
    "        y_pred = np.zeros(X.shape[0])\n",
    "\n",
    "        for alpha, model in zip(self.alphas, self.models):\n",
    "            y_pred += alpha * model.predict(X)\n",
    "\n",
    "        # convert margin to probabilities using sigmoid function\n",
    "        probs = 1 / (1 + np.exp(-2 * y_pred))\n",
    "\n",
    "        return np.vstack([1 - probs, probs]).T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df24f179351fa008",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Generate data\n",
    "Please use the cell below to discuss your dataset choice and why it is appropriate (or not) for this algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c4083f-5267-44d3-89ed-65864f82aa57",
   "metadata": {},
   "source": [
    "We use make_gaussian_quantiles to generate a 2D dataset with two classes. This dataset is appropriate for AdaBoost because it’s low-dimensional and has some class overlap, which tests the algorithm’s ability to improve decision boundaries through boosting. The 2D format also makes the results easy to visualize. Labels are converted to +- 1 to match AdaBoost's expected input format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a14a3b8890e86f9",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# create gaussian quantiles dataset\n",
    "# gaussian one\n",
    "X1, Y1 = make_gaussian_quantiles(\n",
    "    n_samples=2000,\n",
    "    n_features=2,\n",
    "    n_classes=1,\n",
    "    mean=[-0.6, -0.6],\n",
    "    cov=0.4,\n",
    "    random_state=42\n",
    ")\n",
    "Y1 = np.zeros(len(Y1))  # All class 0\n",
    "\n",
    "# gaussian two \n",
    "X2, Y2 = make_gaussian_quantiles(\n",
    "    n_samples=2000,\n",
    "    n_features=2,\n",
    "    n_classes=1,\n",
    "    mean=[0.6, 0.6],\n",
    "    cov=0.4,\n",
    "    random_state=123\n",
    ")\n",
    "Y2 = np.ones(len(Y2))   # All class 1\n",
    "\n",
    "# Combine the two gaussians\n",
    "X = np.vstack([X1, X2])\n",
    "Y = np.hstack([Y1, Y2])\n",
    "\n",
    "# convert labels to -1 and 1\n",
    "Y = np.where(Y == 1, 1, -1)\n",
    "\n",
    "# split the dataset into training and testing sets\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da49bb42f79a55f",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# AdaBoost demonstration \n",
    "Demonstrate your AdaBoost implementation.\n",
    "\n",
    "Add plots and figures. \n",
    "\n",
    "Please use the cell below to describe your results and tests.\n",
    "\n",
    "Describe the difference between your implementation and the sklearn implementation. Hint: you can look at the documentation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a064afb5-aeea-48d8-b315-921bf4f8238f",
   "metadata": {},
   "source": [
    "We train and evaluate our custom AdaBoost implementation and compare it to sklearn’s AdaBoostClassifier. We use the same dataset and compare their performance using accuracy, ROC curves, and decision boundary plots. (Accuracy can be found in top left of the plots)\n",
    "\n",
    "Our custom AdaBoost uses decision stumps and updates weights like sklearn’s AdaBoostClassifier. Both use a learning rate to control each weak learner’s impact. Our custom model stops early if error is above 0.5, while sklearn stops if error is zero or after max rounds. Sklearn has extra optimizations and handles sample weights more flexibly. Our custom version is simpler, while sklearn is made for more general use. (info from sklearn's docs)\n",
    "\n",
    "We also perform a grid search over different numbers of estimators and learning rates for the custom model to find the best configuration, showing how hyperparameter tuning affects accuracy.\n",
    "\n",
    "Then we plotted ROC curves and decision boundaries to illustrate the differences in classifier confidence and decision surfaces between the two implementations on both training and test data.\n",
    "\n",
    "As seen in the decision boundary plots both implemntations manage to classify and seprate the classes well with accuracy above 90% which indicate that custom algorithm was well implemented.\n",
    "In the ROC plots, we observe scores close to 1, suggesting excellent performance in distinguishing between the two classes. This further confirms that the custom AdaBoost behaves similarly to the Scikit-learn implementation in both accuracy and confidence of predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e4c7cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to plot decision boundary\n",
    "def plot_decision_boundary(model, X, Y, acc=None, title=\"\", pca=None, ax=None):\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 300),\n",
    "                         np.linspace(y_min, y_max, 300))\n",
    "    grid = np.c_[xx.ravel(), yy.ravel()]\n",
    "    grid_input = pca.inverse_transform(grid) if pca else grid\n",
    "    Z = model.predict(grid_input).reshape(xx.shape)\n",
    "\n",
    "    if ax:\n",
    "        ax.contourf(xx, yy, Z, alpha=0.2, cmap='bwr')\n",
    "        ax.scatter(X[:, 0], X[:, 1], c=Y, cmap='bwr', edgecolor='k', s=20)\n",
    "        if acc is not None:\n",
    "            ax.text(x_min, y_max, f\"Accuracy: {acc:.2f}\", fontsize=12,\n",
    "            verticalalignment='top', bbox=dict(boxstyle=\"round\", facecolor='white', alpha=0.7))\n",
    "        ax.set_title(title)\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "    else:\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.contourf(xx, yy, Z, alpha=0.2, cmap='bwr')\n",
    "        plt.scatter(X[:, 0], X[:, 1], c=Y, cmap='bwr', edgecolor='k', s=20)\n",
    "        if acc is not None:\n",
    "            plt.text(x_min, y_max, f\"Accuracy: {acc:.2f}\", fontsize=12,\n",
    "                     verticalalignment='top', bbox=dict(boxstyle=\"round\", facecolor='white', alpha=0.7))\n",
    "        plt.title(title)\n",
    "        plt.xlabel(\"Feature 0\")\n",
    "        plt.ylabel(\"Feature 1\")\n",
    "        plt.grid(True)\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b3628856e1335fd",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# HYPERPARAMETER TUNING\n",
    "\n",
    "estimator_range = [1, 5, 10, 25, 50, 75, 100]\n",
    "learning_rates = [0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n",
    "results = []\n",
    "\n",
    "# grid search over T and learning_rate\n",
    "for T, lr in product(estimator_range, learning_rates):\n",
    "    model = AdaBoostCustom(T=T) #, learning_rate=lr)\n",
    "    model.fit(X_train, Y_train)\n",
    "    acc = accuracy_score(Y_test, model.predict(X_test))\n",
    "    results.append((T, lr, acc))\n",
    "\n",
    "df = pd.DataFrame(results, columns=[\"T\", \"lr\", \"accuracy\"])\n",
    "pivot = df.pivot(index=\"T\", columns=\"lr\", values=\"accuracy\")\n",
    "\n",
    "best_idx = np.unravel_index(np.argmax(pivot.values), pivot.shape)\n",
    "best_T = pivot.index[best_idx[0]]\n",
    "best_lr = pivot.columns[best_idx[1]]\n",
    "best_acc = pivot.values[best_idx]\n",
    "\n",
    "# print the best configuration\n",
    "print(f\"\\nBest Custom AdaBoost Configuration:\")\n",
    "print(f\"  - Number of Estimators (T): {best_T}\")\n",
    "print(f\"  - Learning Rate: {best_lr}\")\n",
    "print(f\"  - Test Accuracy: {best_acc:.4f}\")\n",
    "\n",
    "\n",
    "# VISUALIZATION OF DECISION BOUNDARIES\n",
    "\n",
    "# using the best parameters from grid search\n",
    "custom_model = AdaBoostCustom(T=75, learning_rate=0.5)\n",
    "custom_model.fit(X_train, Y_train)\n",
    "\n",
    "sk_model = AdaBoostClassifier(n_estimators=75, learning_rate=0.5, random_state=42)\n",
    "sk_model.fit(X_train, Y_train)\n",
    "\n",
    "# compute accuracies\n",
    "custom_train_acc = accuracy_score(Y_train, custom_model.predict(X_train))\n",
    "custom_test_acc = accuracy_score(Y_test, custom_model.predict(X_test))\n",
    "sk_train_acc = accuracy_score(Y_train, sk_model.predict(X_train))\n",
    "sk_test_acc = accuracy_score(Y_test, sk_model.predict(X_test))\n",
    "\n",
    "# plot decision boundaries\n",
    "plot_decision_boundary(custom_model, X_train, Y_train, custom_train_acc, \"Custom AdaBoost Decision Boundary (Train Data)\")\n",
    "plot_decision_boundary(sk_model, X_train, Y_train, sk_train_acc, \"Sklearn AdaBoost Decision Boundary (Train Data)\")\n",
    "plot_decision_boundary(custom_model, X_test, Y_test, custom_test_acc, \"Custom AdaBoost Decision Boundary (Test Data)\")\n",
    "plot_decision_boundary(sk_model, X_test, Y_test, sk_test_acc, \"Sklearn AdaBoost Decision Boundary (Test Data)\")\n",
    "\n",
    "\n",
    "# PLOT ROC CURVES\n",
    "\n",
    "# plot ROC curves for both custom and sklearn AdaBoost models\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# ROC for custom model\n",
    "custom_proba = custom_model.predict_proba(X_test)[:, 1]\n",
    "custom_fpr, custom_tpr, _ = roc_curve(Y_test == 1, custom_proba)\n",
    "custom_roc_auc = auc(custom_fpr, custom_tpr)\n",
    "plt.plot(custom_fpr, custom_tpr, color='blue', lw=2, \n",
    "         label=f'Custom AdaBoost ROC (area = {custom_roc_auc:.2f})')\n",
    "\n",
    "# ROC for sklearn model\n",
    "sklearn_proba = sk_model.predict_proba(X_test)[:, 1]\n",
    "sklearn_fpr, sklearn_tpr, _ = roc_curve(Y_test == 1, sklearn_proba)\n",
    "sklearn_roc_auc = auc(sklearn_fpr, sklearn_tpr)\n",
    "plt.plot(sklearn_fpr, sklearn_tpr, color='red', lw=2, \n",
    "         label=f'Sklearn AdaBoost ROC (area = {sklearn_roc_auc:.2f})')\n",
    "\n",
    "# plot results\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve Comparison: Custom vs Sklearn AdaBoost')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "73fa2fceedc77e92",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Generate additional data sets\n",
    "Generate at least two experimental datasets with binary labels, designed to demonstrate specific properties of AdaBoost (e.g., handling noise or overfitting).\n",
    "\n",
    "Add plots and figures.\n",
    "\n",
    "Please use the cell below to describe your suggested approach in detail. Use formal notations where appropriate.\n",
    "\n",
    "Describe and discuss your results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b34701c-cc3b-439a-b2d0-393449cf5a20",
   "metadata": {},
   "source": [
    "We generated three datasets to highlight distinct behaviors of AdaBoost: dealing with noise, risk of overfitting, and capturing complex patterns.\n",
    "\n",
    "1. High noise dataset:\n",
    "Created by adding significant label noise and feature overlap, this set challenges AdaBoost’s robustness. The similar performance on training and test sets indicates the model’s ability to focus on hard-to-classify points without overfitting noisy labels.\n",
    "\n",
    "2. Overfitting-prone dataset:\n",
    "With clean data but very high boosting rounds, this dataset exposes AdaBoost’s tendency to overfit by perfectly fitting the training data while losing accuracy on unseen samples, showing some gap between train and test results.\n",
    "\n",
    "3. Non-linear dataset:\n",
    "Based on a noisy two-moons pattern, this dataset tests AdaBoost’s flexibility to learn non-linear boundaries. The balanced accuracy across train and test sets demonstrates effective adaptation to complex class shapes without overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d38dc132b23e7b",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# # Generate additional data sets\n",
    "# and\n",
    "# # Split data sets\n",
    "\n",
    "# dataset 1: noisy labels\n",
    "# gaussian one\n",
    "X1, Y1 = make_gaussian_quantiles(\n",
    "    n_samples=1500,\n",
    "    n_features=2,\n",
    "    n_classes=1,\n",
    "    mean=[-0.6, -0.6],\n",
    "    cov=0.4,\n",
    "    random_state=42\n",
    ")\n",
    "Y1 = np.zeros(len(Y1))  # All class 0\n",
    "\n",
    "# gaussian two \n",
    "X2, Y2 = make_gaussian_quantiles(\n",
    "    n_samples=1500,\n",
    "    n_features=2,\n",
    "    n_classes=1,\n",
    "    mean=[0.6, 0.6],\n",
    "    cov=0.4,\n",
    "    random_state=123\n",
    ")\n",
    "Y2 = np.ones(len(Y2))   # All class 1\n",
    "\n",
    "# Combine the two gaussians\n",
    "X1 = np.vstack([X1, X2])\n",
    "Y = np.hstack([Y1, Y2])\n",
    "\n",
    "# convert labels to -1 and 1\n",
    "Y1 = np.where(Y == 1, 1, -1)\n",
    "\n",
    "# flip labels for noise\n",
    "flip_ratio = 0.2\n",
    "n_flip = int(flip_ratio * len(Y))\n",
    "flip_indices = np.random.choice(len(Y), size=n_flip, replace=False)\n",
    "Y[flip_indices] *= -1\n",
    "\n",
    "X1_train, X1_test, y1_train, y1_test = train_test_split(X1, Y1, test_size=0.2, random_state=42)\n",
    "\n",
    "# dataset 2: high-dimensional and prone to overfitting\n",
    "X2, y2 = make_classification(\n",
    "    n_samples=2000,\n",
    "    n_features=10,\n",
    "    n_informative=2,\n",
    "    n_redundant=3,\n",
    "    n_clusters_per_class=1,\n",
    "    n_classes=2,\n",
    "    flip_y=0.0,\n",
    "    class_sep=1.5,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "y2 = 2 * y2 - 1\n",
    "X2_train, X2_test, y2_train, y2_test = train_test_split(X2, y2, test_size=0.2, random_state=42)\n",
    "\n",
    "# dataset 3: non-linear structure\n",
    "X3, y3 = make_moons(n_samples=3000, noise=0.2, random_state=42)\n",
    "y3 = 2 * y3 - 1\n",
    "X3_train, X3_test, y3_train, y3_test = train_test_split(X3, y3, test_size=0.2, random_state=42)\n",
    "\n",
    "# pca for visualization only\n",
    "pca1 = PCA(n_components=2).fit(X1_test)\n",
    "X1_test_pca = pca1.transform(X1_test)\n",
    "\n",
    "pca2 = PCA(n_components=2).fit(X2_test)\n",
    "X2_test_pca = pca2.transform(X2_test)\n",
    "\n",
    "pca3 = PCA(n_components=2).fit(X3_test)\n",
    "X3_test_pca = pca3.transform(X3_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "567bfcc7-6542-489d-b67a-13004d3103b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = [\n",
    "    (\"Dataset 1\", X1_train, y1_train, X1_test, y1_test, pca1, 50),\n",
    "    (\"Dataset 2\", X2_train, y2_train, X2_test, y2_test, pca2, 5000),\n",
    "    (\"Dataset 3\", X3_train, y3_train, X3_test, y3_test, pca3, 50)\n",
    "]\n",
    "\n",
    "fig, axes = plt.subplots(3, 2, figsize=(12, 10))\n",
    "\n",
    "for i, (label, X_train, y_train, X_test, y_test, pca, T) in enumerate(datasets):\n",
    "    custom = AdaBoostCustom(T=T)\n",
    "    custom.fit(X_train, y_train)\n",
    "    sk_model = AdaBoostClassifier(n_estimators=T, random_state=42)\n",
    "    sk_model.fit(X_train, y_train)\n",
    "\n",
    "    acc_custom = accuracy_score(y_test, custom.predict(X_test))\n",
    "    acc_sk = accuracy_score(y_test, sk_model.predict(X_test))\n",
    "\n",
    "    plot_decision_boundary(\n",
    "        custom, X_test, y_test,\n",
    "        acc=acc_custom,\n",
    "        title=f\"Custom AdaBoost - {label}\",\n",
    "        pca=pca,\n",
    "        ax=axes[i, 0]\n",
    "    )\n",
    "    plot_decision_boundary(\n",
    "        sk_model, X_test, y_test,\n",
    "        acc=acc_sk,\n",
    "        title=f\"Sklearn AdaBoost - {label}\",\n",
    "        pca=pca,\n",
    "        ax=axes[i, 1]\n",
    "    )\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# todo: check all explanations and comments + finish test section + additional datasets"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5667598e-28fa-496e-b04b-ae528e653894",
   "metadata": {},
   "source": [
    "# Test algorithms\n",
    "Test your AdaBoost, a library implementation of AdaBoost and at least two additional models, one of which must be another boosting algorithm on your two datasets.\n",
    "\n",
    "Add plots and figures.\n",
    "\n",
    "Please use the cell below to describe your suggested approach in detail. Use formal notations where appropriate.\n",
    "\n",
    "Describe and discuss your results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f959a354-350d-49b8-b203-bdf889778766",
   "metadata": {},
   "source": [
    "We compared four models on a binary classification task: \n",
    "1. custom AdaBoost\n",
    "2. Scikit-learn’s AdaBoost\n",
    "3. Gradient Boosting\n",
    "4. Random Forest.\n",
    "\n",
    "The custom and Scikit-learn AdaBoost models performed similarly, both producing clear boundaries and balanced train/test accuracy near 90%. This suggests the our custom implementation works well and describes AdaBoost properly.\n",
    "\n",
    "Gradient Boosting showed a smoother decision boundary and achieved slightly lower accuracy compared to AdaBoost. Its focus on minimizing loss directly, rather than reweighting samples like AdaBoost, makes it more stable in the presence of noise and overlapping data.\n",
    "\n",
    "Random Forest achieved perfect accuracy but with overly complex boundaries, pointing to overfitting. Its reliance on deep, unpruned trees and bootstrapping leads to high variance if not regularized.\n",
    "\n",
    "The ROC and accuracy plots support these findings—highlighting Gradient Boosting’s strong generalization and AdaBoost’s close alignment between theory and practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e4c3e86-003e-4812-8b36-65e13fccc4c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up models \n",
    "\n",
    "# create gaussian quantiles dataset\n",
    "# gaussian one\n",
    "X1, Y1 = make_gaussian_quantiles(\n",
    "    n_samples=1500,\n",
    "    n_features=2,\n",
    "    n_classes=1,\n",
    "    mean=[-0.5, -0.5],\n",
    "    cov=0.8,\n",
    "    random_state=42\n",
    ")\n",
    "Y1 = np.zeros(len(Y1))  # All class 0\n",
    "\n",
    "# gaussian two \n",
    "X2, Y2 = make_gaussian_quantiles(\n",
    "    n_samples=1500,\n",
    "    n_features=2,\n",
    "    n_classes=1,\n",
    "    mean=[0.5, 0.5],\n",
    "    cov=0.5,\n",
    "    random_state=123\n",
    ")\n",
    "Y2 = np.ones(len(Y2))   # All class 1\n",
    "\n",
    "# Combine the two gaussians\n",
    "X = np.vstack([X1, X2])\n",
    "Y = np.hstack([Y1, Y2])\n",
    "\n",
    "# convert labels to -1 and 1\n",
    "Y = np.where(Y == 1, 1, -1)\n",
    "\n",
    "# split the dataset into training and testing sets\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "# models configuration\n",
    "models = {\n",
    "    'Custom AdaBoost': lambda: AdaBoostCustom(T=75, learning_rate=0.5),\n",
    "    'Sklearn AdaBoost': lambda: AdaBoostClassifier(n_estimators=75, learning_rate=0.5, random_state=42),\n",
    "    'Random Forest': lambda: RandomForestClassifier(n_estimators=75, random_state=42),\n",
    "    'Gradient Boosting': lambda: GradientBoostingClassifier(n_estimators=75, learning_rate=0.5, random_state=42)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d097c906-f362-4084-9ee8-0ff34568d53a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test and visualize\n",
    "results = {}\n",
    "\n",
    "for name, ctor in models.items():\n",
    "    model = ctor()\n",
    "    model.fit(X_train, Y_train)\n",
    "    train_acc = accuracy_score(Y_train, model.predict(X_train))\n",
    "    test_acc = accuracy_score(Y_test, model.predict(X_test))\n",
    "    proba = model.predict_proba(X_test)[:, 1]\n",
    "    fpr, tpr, _ = roc_curve(Y_test == 1, proba)\n",
    "    auc_score = auc(fpr, tpr)\n",
    "    results[name] = {\n",
    "        'train_acc': train_acc,\n",
    "        'test_acc': test_acc,\n",
    "        'fpr': fpr,\n",
    "        'tpr': tpr,\n",
    "        'auc': auc_score\n",
    "    }\n",
    "\n",
    "# plot decision boundaries for each model\n",
    "trained = {name: ctor() for name, ctor in models.items()}\n",
    "for name, model in trained.items():\n",
    "    model.fit(X_train, Y_train)\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "for ax, (name, model) in zip(axes.ravel(), trained.items()):\n",
    "    acc = results[name]['test_acc']\n",
    "    auc_score = results[name]['auc']\n",
    "    plot_decision_boundary(\n",
    "        model, X_train, Y_train, acc=acc,\n",
    "        title=f\"{name}\\nTest Acc={acc:.2f}, AUC={auc_score:.2f}\",\n",
    "        ax=ax\n",
    "    )\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# plot ROC curve for each model\n",
    "plt.figure(figsize=(8,6))\n",
    "for name, res in results.items():\n",
    "    plt.plot(res['fpr'], res['tpr'], label=f\"{name} (AUC={res['auc']:.2f})\")\n",
    "plt.plot([0,1],[0,1],'k--')\n",
    "plt.xlabel(\"False Positive Rate\"); plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC Curves\")\n",
    "plt.legend(loc=\"lower right\"); plt.grid(True); plt.tight_layout(); plt.show()\n",
    "\n",
    "# plot comparison of train and test accuracies and compare models\n",
    "labels = list(models.keys())\n",
    "train_accs = [results[n]['train_acc'] for n in labels]\n",
    "test_accs  = [results[n]['test_acc']  for n in labels]\n",
    "x = np.arange(len(labels))\n",
    "width = 0.35\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.bar(x - width/2, train_accs, width, label='Train Acc')\n",
    "plt.bar(x + width/2, test_accs,  width, label='Test Acc')\n",
    "plt.xticks(x, labels, rotation=45)\n",
    "plt.ylabel(\"Accuracy\"); plt.title(\"Train vs Test Accuracy for Each Model\")\n",
    "plt.legend(); plt.grid(axis='y'); plt.tight_layout(); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18c95c7f-d3a9-4e3d-b539-02e020358766",
   "metadata": {},
   "source": [
    "# Use of generative AI\n",
    "Please use the cell below to describe your use of generative AI in this assignment. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36753fd7-8b2d-487b-82ae-dc318eca3ca6",
   "metadata": {},
   "source": [
    "We used generative AI tools such as ChatGPT and Claude to assist with technical aspects like using Python packages, improving code readability, and debugging issues. We also used them for creating visualizations and making the code more efficient. For example, helping us to find the right NumPy operations to optimize performance. The core algorithm design, theoretical understanding, data processing, and full implementation were based on material taught in class and additional study from online sources."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python .venv",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
