{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10316d5779a3733",
   "metadata": {
    "collapsed": false,
    "id": "10316d5779a3733",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Exercise 1: t-SNE\n",
    "\n",
    "## Do not start the exercise until you fully understand the submission guidelines.\n",
    "\n",
    "\n",
    "* The homework assignments are executed automatically.\n",
    "* Failure to comply with the following instructions will result in a significant penalty.\n",
    "* Appeals regarding your failure to read these instructions will be denied.\n",
    "\n",
    "## Read the following instructions carefully:\n",
    "\n",
    "1. This Jupyter notebook contains all the step-by-step instructions needed for this exercise.\n",
    "1. Write **efficient**, **vectorized** code whenever possible. Some calculations in this exercise may take several minutes when implemented efficiently, and might take much longer otherwise. Unnecessary loops will result in point deductions.\n",
    "1. You are responsible for the correctness of your code and should add as many tests as you see fit to this jupyter notebook. Tests will not be graded nor checked.\n",
    "1. You are allowed to use functions and methods from the [Python Standard Library](https://docs.python.org/3/library/).\n",
    "1. Your code must run without errors. Use at least `numpy` 1.15.4. Any code that cannot run will not be graded.\n",
    "1. Write your own code. Cheating will not be tolerated.\n",
    "1. Submission includes a zip file that contains this notebook, with your ID as the file name. For example, `hw1_123456789_987654321.zip` if you submitted in pairs and `hw1_123456789.zip` if you submitted the exercise alone. The name of the notebook should follow the same structure.\n",
    "   \n",
    "Please use only a **zip** file in your submission.\n",
    "\n",
    "---\n",
    "##❗❗❗❗❗❗❗❗❗**This is mandatory**❗❗❗❗❗❗❗❗❗\n",
    "## Please write your RUNI emails in this cell:\n",
    "\n",
    "### *** YOUR EMAILS HERE ***\n",
    "\n",
    "---\n",
    "\n",
    "## Please sign that you have read and understood the instructions:\n",
    "\n",
    "### *** YOUR IDS HERE ***\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "735832cbfa43f83",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-26T15:28:04.108350Z",
     "start_time": "2025-04-26T15:28:02.664002Z"
    },
    "collapsed": false,
    "id": "735832cbfa43f83",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import pairwise_distances as p_distances\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.manifold import trustworthiness\n",
    "import numba as nb\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import trange\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f877cca-f7d1-41ca-9d3d-28d587bec85c",
   "metadata": {
    "id": "8f877cca-f7d1-41ca-9d3d-28d587bec85c"
   },
   "source": [
    "# Design your algorithm\n",
    "Make sure to describe the algorithm, its limitations, and describe use-cases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ba7760b-b4ef-45a9-9aad-88b45fd8d442",
   "metadata": {
    "id": "0ba7760b-b4ef-45a9-9aad-88b45fd8d442"
   },
   "source": [
    "The algorithm t-SNE is a dimensionality reduction technique used to visualize high-dimensional data in 2D or 3D and can also be used in fields such as bioinformatics. It models pairwise similarities in high-dimensional space using Gaussians and maps them to a lower-dimensional space using a t-distribution. The algorithm minimizes KL divergence between the two distributions to preserve local structures. Its main goal is to keep similar data points close together in the lower-dimensional space.\n",
    "\n",
    "The t-SNE algorithm relies on pairwise distances and an iterative process where gradients are computed to minimize the KL divergence. At each step, our implementation of the algorithm updates the positions of the points in the low-dimensional space, using momentum-based gradient descent to ensure efficient optimization. A key part of the algorithm is selecting the right hyperparameters, especially the perplexity, which controls the number of nearest neighbors that influence the probability distributions as well as the learning rate. Also, we initialized the Y values using PCA to ensure faster convergence and reduce the risk of local minima opposed to using random values. Moreover, choosing an appropriate momentum helps the algorithm converge faster.\n",
    "\n",
    "However, t-SNE has some limitations; it is computationally expensive and does not scale well to very large datasets, as it has a time complexity that is approximately quadratic in the number of data points. Also, it can be sensitive to the choice of hyperparameters, and the results can vary significantly based on the initialization of the low-dimensional representation. Additionally, t-SNE does not preserve global structures well, which can lead to misleading interpretations if not used carefully.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2158ff2629daf2bb",
   "metadata": {
    "collapsed": false,
    "id": "2158ff2629daf2bb",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Your implementations\n",
    "You may add new cells, write helper functions or test code as you see fit.\n",
    "Please use the cell below and include a description of your implementation.\n",
    "Explain code design consideration, algorithmic choices and any other details you think is relevant to understanding your implementation.\n",
    "Failing to explain your code will lead to point deductions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe206c9-d1f4-440b-aca0-c807cdd79451",
   "metadata": {
    "id": "afe206c9-d1f4-440b-aca0-c807cdd79451"
   },
   "source": [
    "In this implementation of t-SNE, we have followed the core principles of the algorithm. fit_transform() is the main method of the algorithm, responsible for the optimization process. Inside this method, the pairwise affinities are computed for the high-dimensional space using compute_pairwise_affinities(), which calculates Gaussian-based affinities based on pairwise distances. This helps in setting up the high-dimensional probability distribution P{ij} for each data point.\n",
    "\n",
    "After the high-dimensional affinities are computed, we proceed with an iterative optimization procedure. For each iteration, the pairwise affinities in the low-dimensional space are computed using the compute_pairwise_t_distribution_affinities() method, which uses t-distribution. The gradient we implemented is momentum-based and the momentum term helps speed-up convergence, adjusting its value based on whether the iteration has reached the momentum_switch_iter.\n",
    "\n",
    "We used binary search in binary_search_precision() to determine the optimal gaussian_precision (standard deviation) value, which controls the precision of the Gaussian distribution in the high-dimensional space. This search ensures that the target entropy (determined by the perplexity) is achieved.\n",
    "\n",
    "The code structure makes sure to handle the computational complexity efficiently by vectorizing operations (numpy) where possible and avoiding unnecessary recomputations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b85d8f7447ebce0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-26T15:28:04.133175Z",
     "start_time": "2025-04-26T15:28:04.120075Z"
    },
    "collapsed": false,
    "id": "7b85d8f7447ebce0",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "class CustomTSNE:\n",
    "    def __init__(self, perplexity=30.0, n_components=2, n_iter=1000, learning_rate=200.0,\n",
    "                 momentum_initial=0.5, momentum_final=0.9, momentum_switch_iter=250, tolerance=1e-5, epsilon=1e-10):\n",
    "        self.perplexity = perplexity\n",
    "        self.n_components = n_components\n",
    "        self.n_iter = n_iter\n",
    "        self.learning_rate = learning_rate\n",
    "        self.momentum_initial = momentum_initial\n",
    "        self.momentum_final = momentum_final\n",
    "        self.momentum_switch_iter = momentum_switch_iter\n",
    "        self.tolerance = tolerance\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    # Part 1: Implementing t-SNE\n",
    "        # Step 1: Compute pairwise affinities in the original space with a Gaussian distribution\n",
    "\n",
    "    def fit_transform(self, X):\n",
    "        \"\"\"\n",
    "        t-SNE algorithm implementation\n",
    "        :param X: data points in high-dimensional space\n",
    "        :return Y: 2D representation of the data points\n",
    "        \"\"\"\n",
    "        n_samples = X.shape[0]\n",
    "        p_affinities = self._compute_pairwise_affinities(X)\n",
    "\n",
    "        # random initialization of Y\n",
    "        # Y = np.random.randn(n_samples, 2)\n",
    "\n",
    "        # PCA initialization + normalization\n",
    "        pca = PCA(n_components=self.n_components)\n",
    "        Y = pca.fit_transform(X)\n",
    "        min_y = np.min(Y)\n",
    "        max_y = np.max(Y)\n",
    "        Y = (Y - min_y) / np.maximum(max_y - min_y, self.epsilon)\n",
    "        V = np.zeros_like(Y)\n",
    "\n",
    "        # early stopping\n",
    "        current_kl = np.inf\n",
    "        current_iter = 0\n",
    "\n",
    "        # momentum gradient descent process\n",
    "        with trange(self.n_iter, desc=\"t-SNE Optimization\") as progress:\n",
    "            for i in progress:\n",
    "                q_affinities, inverse_pairwise_distances = self._compute_pairwise_t_distribution_affinities(Y)\n",
    "                kl = np.sum(p_affinities * np.log2(np.maximum(p_affinities, self.epsilon) / np.maximum(q_affinities, self.epsilon)))\n",
    "\n",
    "                progress.set_postfix({\"KL Divergence\": f\"{kl:.4f}\"})\n",
    "\n",
    "                pq_affinities = p_affinities - q_affinities\n",
    "\n",
    "                gradient_Y = np.zeros_like(Y)\n",
    "                for j in range(n_samples):\n",
    "                    # compute gradient for each point\n",
    "                    gradient_Y[j] = 4 * np.sum(\n",
    "                        pq_affinities[j, :, None] *\n",
    "                        inverse_pairwise_distances[j, :, None] *\n",
    "                        (Y[j] - Y), axis=0)\n",
    "\n",
    "                # update momentum\n",
    "                if i < self.momentum_switch_iter:\n",
    "                    momentum = self.momentum_initial\n",
    "                else:\n",
    "                    momentum = self.momentum_final\n",
    "\n",
    "                # update Y with momentum\n",
    "                V = momentum * V - self.learning_rate * gradient_Y\n",
    "                Y += V\n",
    "\n",
    "                if self.tolerance < current_kl - kl:\n",
    "                    current_kl = kl\n",
    "                    current_iter = i\n",
    "                elif i - current_iter > self.momentum_switch_iter:\n",
    "                    print(f\"Early stop at: {i}, KL divergence: {current_kl:.3f}\")\n",
    "                    break\n",
    "\n",
    "        return Y\n",
    "\n",
    "    def _compute_pairwise_t_distribution_affinities(self, Y):\n",
    "        \"\"\"\n",
    "        compute pairwise affinities in the low-dimensional space using t-distribution\n",
    "        :param Y: data points in low-dimensional space\n",
    "        :return q_affinities: pairwise affinities in low-dimensional space\n",
    "        :return inverse_pairwise_distances: pairwise distances in low-dimensional space\n",
    "        \"\"\"\n",
    "        # pairwise distances\n",
    "        distances = p_distances(Y, squared=True)\n",
    "        inverse_distances = 1 / (1 + distances)\n",
    "\n",
    "        # set diagonal to 0 for self-affinity\n",
    "        np.fill_diagonal(inverse_distances, 0)\n",
    "\n",
    "        # compute q affinities for t-distribution\n",
    "        q_affinities = np.maximum(inverse_distances / np.sum(inverse_distances), self.epsilon)\n",
    "\n",
    "        return q_affinities, inverse_distances\n",
    "\n",
    "    def _compute_pairwise_affinities(self, X):\n",
    "        \"\"\"\n",
    "        compute pairwise affinities in the original space using Gaussian distribution\n",
    "        :param X: data points in high-dimensional space\n",
    "        :return p_affinities: pairwise affinities in high-dimensional space\n",
    "        \"\"\"\n",
    "        # pairwise distances\n",
    "        distances = p_distances(X, squared=True)\n",
    "        np.fill_diagonal(distances, 0)\n",
    "\n",
    "        # init pairwise affinities\n",
    "        n_samples = X.shape[0]\n",
    "        p_affinities = np.zeros((n_samples, n_samples))\n",
    "\n",
    "        # entropy based on perplexity\n",
    "        target_entropy = np.log2(self.perplexity)\n",
    "\n",
    "        # compute affinities for each data point\n",
    "        for i in range(n_samples):\n",
    "            p_i = self._binary_search_precision(distances, target_entropy, i)\n",
    "            p_affinities[i] = p_i\n",
    "            p_affinities[i, i] = 0 # zero out the self-affinity\n",
    "\n",
    "        # symmetric SNE\n",
    "        p_affinities = (p_affinities + p_affinities.T) / (2 * n_samples)\n",
    "\n",
    "        return p_affinities\n",
    "\n",
    "    def _binary_search_precision(self, distances, target_entropy, i):\n",
    "        \"\"\"\n",
    "        binary search to find the correct precision for Gaussian distribution\n",
    "        :param distances: pairwise distances\n",
    "        :param target_entropy: target entropy based on perplexity\n",
    "        :param i: index of the data point\n",
    "        :return p_affinities: pairwise affinities for the data point\n",
    "        \"\"\"\n",
    "        precision = 1.0\n",
    "        min_precision, max_precision = -np.inf, np.inf\n",
    "        p_affinities = np.zeros_like(distances[i])\n",
    "\n",
    "        # binary search to find the correct precision\n",
    "        for j in range(50):\n",
    "            # compute affinities for point i\n",
    "            p_affinities = np.exp(-distances[i] / (2 * precision ** 2))\n",
    "            p_affinities[i] = 0  # zero out the self-affinity\n",
    "\n",
    "            p_sum = np.sum(p_affinities)\n",
    "            if p_sum == 0:\n",
    "                p_sum = self.epsilon\n",
    "            p_affinities /= p_sum\n",
    "\n",
    "            # similarity in high-dimensional space\n",
    "            entropy = -np.sum(p_affinities * np.log2(p_affinities + self.epsilon))\n",
    "\n",
    "            # convergence check\n",
    "            if np.abs(entropy - target_entropy) < self.tolerance:\n",
    "                break\n",
    "\n",
    "            # adjust precision based on entropy\n",
    "            if entropy < target_entropy:\n",
    "                min_precision = precision\n",
    "                precision = (min_precision + max_precision) / 2 if max_precision != np.inf else precision * 2\n",
    "            else:\n",
    "                max_precision = precision\n",
    "                precision = (max_precision + min_precision) / 2 if min_precision != 0 else precision / 2\n",
    "\n",
    "        return p_affinities\n",
    "\n",
    "    def transform(self, X_original, Y_original, X_new):\n",
    "        \"\"\"\n",
    "        Transform new data points into the low-dimensional space using the learned mapping\n",
    "        :param X_original:\n",
    "        :param Y_original:\n",
    "        :param X_new:\n",
    "        :return: newly transformed data points in low-dimensional space\n",
    "        \"\"\"\n",
    "        # use nearest neighbors to find the closest points in the original space\n",
    "        n_neighbors = max(int(self.perplexity), 2)\n",
    "        knn = NearestNeighbors(n_neighbors=n_neighbors)\n",
    "        knn.fit(X_original)\n",
    "\n",
    "        # calculate distances and indices of neighbors\n",
    "        distances, indices = knn.kneighbors(X_new)\n",
    "        precision = np.std(distances, axis=1, keepdims=True) + self.epsilon\n",
    "\n",
    "        # compute weights based on Gaussian distribution\n",
    "        weights = np.exp(-distances ** 2 / (2 * precision ** 2))\n",
    "        row_sums = np.sum(weights, axis=1, keepdims=True)\n",
    "        row_sums[row_sums == 0] = 1  # prevent division by zero\n",
    "        weights /= row_sums\n",
    "\n",
    "        # compute the new low-dimensional representation\n",
    "        Y_neighbors = Y_original[indices]\n",
    "        Y_new = np.einsum('ij,ijk->ik', weights, Y_neighbors)\n",
    "\n",
    "        return Y_new\n",
    "\n",
    "    def neighborhood_preservation(self, X_train, Y_train, X_new, Y_new):\n",
    "        \"\"\"\n",
    "        check the neighborhood preservation of the learned mapping\n",
    "        :param X_train:\n",
    "        :param Y_train:\n",
    "        :param X_new:\n",
    "        :param Y_new:\n",
    "        :return: neighborhood preservation score\n",
    "        \"\"\"\n",
    "        # find kNN in high-dimensional space\n",
    "        knn_high = NearestNeighbors(n_neighbors=max(int(self.perplexity), 2))\n",
    "        knn_high.fit(X_train)\n",
    "        neighbors_high = knn_high.kneighbors(X_new, return_distance=False)\n",
    "\n",
    "        # find kNN in low-dimensional space\n",
    "        knn_low = NearestNeighbors(n_neighbors=max(int(self.perplexity), 2))\n",
    "        knn_low.fit(Y_train)\n",
    "        neighbors_low = knn_low.kneighbors(Y_new, return_distance=False)\n",
    "\n",
    "        total = 0\n",
    "        for high, low in zip(neighbors_high, neighbors_low):\n",
    "            intersection = len(set(high) & set(low))\n",
    "            total += intersection / max(int(self.perplexity), 2)\n",
    "\n",
    "        nps = total / X_new.shape[0]\n",
    "\n",
    "        return nps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df24f179351fa008",
   "metadata": {
    "collapsed": false,
    "id": "df24f179351fa008",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Load data\n",
    "Please use the cell below to discuss your dataset choice and why it is appropriate (or not) for this algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c4083f-5267-44d3-89ed-65864f82aa57",
   "metadata": {
    "id": "74c4083f-5267-44d3-89ed-65864f82aa57"
   },
   "source": [
    "We choose to use the MNIST dataset which is appropriate for t-SNE because it contains high-dimensional image data (784 features per sample), where visualizing the raw space is impossible. MNIST has a clear class structure which is ideal for observing how well t-SNE preserves local neighborhoods. Each image represents a handwritten digit (0–9), and t-SNE helps reduce this complex space into 2D while preserving local similarities. We are using a portion of the dataset (2000 samples) as tsne works better with smaller subsets of data. We experimented with various dataset sizes and we found that 2000 samples is balanced in terms of visual result and runtime efficiency. For example, with 500 samples we the results fast and saw similar separation for different clusters, but the distances inside each cluster are more significant. On the other hand, we saw that with 10,000 samples, for example, it takes a significant time to run, but the clusters are visually more dense. Furthermore, each class in MNIST is relatively balanced, but certain digits (e.g. 1 and 7) may overlap, which helps testing t-SNE’s ability to separate visually similar classes. Using PCA before t-SNE already takes care of normalizing and scaling and so normalizing the data beforehand does not affect the results.\n",
    "\n",
    "Behavior on MNIST (2000 samples):\n",
    "- t-SNE often reveals tight clusters for digits like 0, 1, and 6\n",
    "- Some classes (for example 4 and 9 or 5 and 3) show partial overlap, reflecting visual similarity and t-SNE's focus on local, not global, structure.\n",
    "\n",
    "In summary, MNIST enables to demonstrate how well t-SNE captures local structure in complex data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a14a3b8890e86f9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-26T15:28:06.584502Z",
     "start_time": "2025-04-26T15:28:04.226956Z"
    },
    "collapsed": false,
    "id": "a14a3b8890e86f9",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Load data\n",
    "print(\"Loading MNIST dataset...\")\n",
    "mnist = fetch_openml(\"mnist_784\", as_frame=False)  # version=1)\n",
    "X = mnist.data\n",
    "Y = mnist.target\n",
    "\n",
    "# Split the data into train and test\n",
    "print(\"Splitting data...\")\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "X_train_subset = X_train[:2000]\n",
    "Y_train_subset = Y_train[:2000]\n",
    "x_test_subset = X_test[:2000]\n",
    "Y_test_subset = Y_test[:2000]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da49bb42f79a55f",
   "metadata": {
    "collapsed": false,
    "id": "da49bb42f79a55f",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# t-SNE demonstration\n",
    "Demonstrate your t-SNE implementation.\n",
    "\n",
    "Add plots and figures. The code below is just to help you get started, and should not be your final submission.\n",
    "\n",
    "Please use the cell below to describe your results and tests.\n",
    "\n",
    "Describe the difference between your implementation and the sklearn implementation. Hint: you can look at the documentation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a064afb5-aeea-48d8-b315-921bf4f8238f",
   "metadata": {
    "id": "a064afb5-aeea-48d8-b315-921bf4f8238f"
   },
   "source": [
    "In this demonstration, we applied our custom t-SNE implementation and compared it against the standard sklearn t-SNE on a subset of 2000 MNIST samples. To ensure a fair comparison, we performed a hyperparameter search over different perplexity values (5 to 50) as well as learning rate values (100 to 500), number of iterations (850 found to be best balance) and evaluated the quality of the embeddings using the nearest neighbor preservation (trustworthiness) metric. We found that perplexity values around 35 and learning rate around 250 yielded the best trustworthiness scores for both the custom and sklearn versions, indicating that the local neighborhood structure was well-preserved in the low-dimensional space.\n",
    "\n",
    "Generally, both implementations behaved similarly, but there are some differences. Our custom t-SNE manually handles the affinity computation, gradient updates, and optimization process with PCA-based initialization, while sklearn’s t-SNE includes more internal optimizations such as adaptive learning rates and Barnes-Hut optimization. As a result, sklearn’s version is typically more stable and faster for large datasets. However, after tuning the perplexity and the learning rate, our custom t-SNE achieved comparable neighborhood preservation and produced visually similar 2D embeddings.\n",
    "\n",
    "Visual description of the results:\n",
    "The custom t-SNE separates well the MNIST digits, with clear and tight clusters, but the groups are a bit closer together and sometimes slightly overlap. In comparison, the sklearn t-SNE spreads the clusters out more naturally, leaving bigger gaps between different digits and creating more clear shapes. Both versions successed in separating the digits, but sklearn’s version looks cleaner overall, mainly because of small internal optimizations like early exaggeration tuning (as described in the documentation). Overall, our custom t-SNE is working well and with some more small modifications we could get even closer to the sklearn output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b3628856e1335fd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-26T15:59:44.144589Z",
     "start_time": "2025-04-26T15:28:06.593066Z"
    },
    "collapsed": false,
    "id": "9b3628856e1335fd",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Run your custom t-SNE implementation\n",
    "custom_tsne = CustomTSNE(n_components=2, perplexity=30, learning_rate=300, n_iter=850)\n",
    "custom_Y = custom_tsne.fit_transform(X_train_subset)\n",
    "\n",
    "# Run sklearn t-SNE\n",
    "sk_tsne = TSNE(n_components=2, init='pca', perplexity=30)\n",
    "sk_Y = sk_tsne.fit_transform(X_train_subset)\n",
    "\n",
    "# Visualization of the result\n",
    "plt.figure()\n",
    "plt.scatter(custom_Y[:, 0], custom_Y[:, 1], s=5, c=Y_train_subset.astype(int), cmap='tab10')\n",
    "plt.scatter(custom_Y[:, 0], custom_Y[:, 1], s=5, c=Y_train_subset.astype(int), cmap='tab10')\n",
    "plt.colorbar()\n",
    "plt.title('MNIST Data Embedded into 2D with Custom t-SNE')\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(sk_Y[:, 0], sk_Y[:, 1], s=5, c=Y_train_subset.astype(int), cmap='tab10')\n",
    "plt.colorbar()\n",
    "plt.title('MNIST Data Embedded into 2D with sklearn t-SNE')\n",
    "plt.show()\n",
    "\n",
    "# optimizing the hyperparameters values (uncomment to run optimization)\n",
    "\n",
    "# perplexities = list(range(25, 45, 5))\n",
    "# learning_rates = list(range(200, 400, 50))\n",
    "#\n",
    "# trust_scores = np.zeros((len(learning_rates), len(perplexities)))\n",
    "#\n",
    "# # running all combinations of perpelexity and learning rate values\n",
    "# for i, lr in enumerate(learning_rates):\n",
    "#     for j, p in enumerate(perplexities):\n",
    "#         print(f\"Learning rate = {lr}, Perplexity = {p}\")\n",
    "#         custom_tsne = CustomTSNE(n_components=2, perplexity=p, learning_rate=lr, n_iter=850)\n",
    "#         Y_custom = custom_tsne.fit_transform(X_train_subset)\n",
    "#         trust_cust = trustworthiness(X_train_subset, Y_custom, n_neighbors=5)\n",
    "#         trust_scores[i, j] = trust_cust\n",
    "#         print(f\"Trustworthiness = {trust_cust:.4f}\")\n",
    "#\n",
    "# # plotting the results\n",
    "# plt.figure(figsize=(10, 7))\n",
    "# plt.imshow(trust_scores, interpolation='nearest', cmap='viridis')\n",
    "# plt.colorbar(label='Trustworthiness')\n",
    "# plt.xticks(ticks=np.arange(len(perplexities)), labels=perplexities)\n",
    "# plt.yticks(ticks=np.arange(len(learning_rates)), labels=learning_rates)\n",
    "# plt.xlabel('Perplexity')\n",
    "# plt.ylabel('Learning Rate')\n",
    "# plt.title('Trustworthiness Heatmap (Custom t-SNE)')\n",
    "# plt.tight_layout()\n",
    "#\n",
    "# for i in range(len(learning_rates)):\n",
    "#     for j in range(len(perplexities)):\n",
    "#         text = f\"{trust_scores[i, j]:.3f}\"\n",
    "#         plt.text(j, i, text, ha='center', va='center', color='white', fontsize=8)\n",
    "#\n",
    "# plt.show()\n",
    "#\n",
    "# best_idx = np.unravel_index(np.argmax(trust_scores), trust_scores.shape)\n",
    "# best_lr = learning_rates[best_idx[0]]\n",
    "# best_perplexity = perplexities[best_idx[1]]\n",
    "# best_trustworthiness = trust_scores[best_idx]\n",
    "#\n",
    "# print(f\"\\nBest combination:\")\n",
    "# print(f\"Perplexity = {best_perplexity}\")\n",
    "# print(f\"Learning Rate = {best_lr}\")\n",
    "# print(f\"Trustworthiness = {best_trustworthiness:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73fa2fceedc77e92",
   "metadata": {
    "collapsed": false,
    "id": "73fa2fceedc77e92",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# t-SNE extension - mapping new samples\n",
    "Demonstrate your t-SNE transformation procedure.\n",
    "\n",
    "Add plots and figures.\n",
    "\n",
    "Please use the cell below t describe your suggested approach in detail. Use formal notations where appropriate.\n",
    "Describe and discuss your results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b34701c-cc3b-439a-b2d0-393449cf5a20",
   "metadata": {
    "id": "7b34701c-cc3b-439a-b2d0-393449cf5a20"
   },
   "source": [
    "In order to transform new high-dimensional points into the learned low-dimensional space, we used a local interpolation approach based on nearest neighbors.\n",
    "\n",
    "Given a new point $x_{\\text{new}} \\in \\mathbb{R}^D$:\n",
    "\n",
    "1. Neighbor search: We identified its k-nearest neighbors in the original high-dimensional space $X_{\\text{train}}$ using Euclidean distance, where $k = perplexity$.\n",
    "2. Local scale estimation: We computed the standard deviation $\\sigma$ of the distances between $x_{\\text{new}}$ and its neighbors to capture the local neighborhood scale:\n",
    "$\\sigma = \\text{std}\\left(\\{ \\|x_{\\text{new}} - x_i\\| \\}_{i=1}^k\\right) + \\epsilon$\n",
    "where $\\epsilon$ is a small constant to prevent division by zero.\n",
    "3. Weight calculation:\n",
    "We assigned a weight to each neighbor based on a Gaussian kernel applied to the squared distances:\n",
    "$w_i = \\exp\\left( -\\frac{\\| x_{\\text{new}} - x_i \\|^2}{2\\sigma^2} \\right)$\n",
    "The weights were then normalized:\n",
    "$\\frac{w_i}{\\sum_j w_j}$\n",
    "4. Low-dimensional interpolation:\n",
    "The final embedding $y_{\\text{new}} \\in \\mathbb{R}^2$ was computed as the weighted sum of the corresponding neighbor embeddings in the low-dimensional space $Y_{\\text{train}}$:\n",
    "$y_{\\text{new}} = \\sum_{i=1}^k w_i \\, y_{\\text{train},i}$\n",
    "\n",
    "From the visualizations, we observe that the transformed test points generally align well with the corresponding training clusters, suggesting that the local interpolation method is effective. However, small deviations and slight misplacements occur in some classes, particularly for more overlapping digits like '2', '9' reflecting the challenge of preserving neighborhood structures exactly after projection.\n",
    "\n",
    "Since the goal of the transformation is to map new data points into the low-dimensional space while preserving local neighborhood relationships, the performance measure we propose is the Neighborhood Preservation Score (NPS).\n",
    "\n",
    "The NPS is defined as the ratio of the number of correctly preserved neighbors in the low-dimensional space to the total number of neighbors in the high-dimensional space. The value of NPS ranges from 0 to 1, where:\n",
    "- $1$ = perfect preservation of neighborhood structure\n",
    "- $0$ = no local structure preserved\n",
    "\n",
    "Similar NPS scores between our custom t-SNE(0.4245) and sklearn(0.4194) in both training and testing suggest that our extension preserves local relationships well, making the method effective for transforming new data points without retraining t-SNE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d38dc132b23e7b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-26T15:59:44.834076Z",
     "start_time": "2025-04-26T15:59:44.167917Z"
    },
    "collapsed": false,
    "id": "9d38dc132b23e7b",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Transform new data\n",
    "custom_Y_new = custom_tsne.transform(X_train_subset, custom_Y, x_test_subset)\n",
    "# custom_Y_sk = custom_tsne.transform(X_train_subset, sk_Y, x_test_subset)\n",
    "\n",
    "# Visualization of the result\n",
    "plt.figure()\n",
    "plt.scatter(custom_Y[:, 0], custom_Y[:, 1], s=5, c=Y_train_subset.astype(int), cmap='tab10')\n",
    "plt.scatter(custom_Y_new[:, 0], custom_Y_new[:, 1], marker = '*', s=50, linewidths=0.5, edgecolors='k', c=Y_test_subset.astype(int), cmap='tab10')\n",
    "plt.colorbar()\n",
    "plt.title('MNIST Data Embedded into 2D with Custom t-SNE')\n",
    "\n",
    "# Visualization of the result for each label\n",
    "fig, axes = plt.subplots(5, 2, figsize=(16, 40))  # 5 rows, 2 columns, much taller figure\n",
    "axes = axes.flatten()\n",
    "\n",
    "for label in range(10):\n",
    "    ax = axes[label]\n",
    "    ax.scatter(custom_Y[Y_train_subset == str(label), 0], custom_Y[Y_train_subset == str(label), 1], s=5, c=Y_train_subset[Y_train_subset == str(label)].astype(int), cmap='tab10', vmin=0, vmax=9, label='Train')\n",
    "    ax.scatter(custom_Y_new[Y_test_subset == str(label), 0], custom_Y_new[Y_test_subset == str(label), 1], marker='*', s=50, linewidths=0.5, edgecolors='k', c=Y_test_subset[Y_test_subset == str(label)].astype(int), cmap='tab10', vmin=0, vmax=9, label='Test')\n",
    "    ax.set_title(f'Label {label}', fontsize=14)\n",
    "    ax.legend(fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# check the neighborhood preservation score (NPS)\n",
    "\n",
    "# nps_score = custom_tsne.neighborhood_preservation(X_train_subset, custom_Y, x_test_subset, custom_Y_new)\n",
    "# print(f\"Neighborhood Preservation Score: {nps_score:.4f}\")\n",
    "# nps_score_sk = custom_tsne.neighborhood_preservation(X_train_subset, sk_Y, x_test_subset, custom_Y_sk)\n",
    "# print(f\"Neighborhood Preservation Score (sklearn): {nps_score_sk:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18c95c7f-d3a9-4e3d-b539-02e020358766",
   "metadata": {
    "id": "18c95c7f-d3a9-4e3d-b539-02e020358766"
   },
   "source": [
    "# Use of generative AI\n",
    "Please use the cell below to describe your use of generative AI in this assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36753fd7-8b2d-487b-82ae-dc318eca3ca6",
   "metadata": {
    "id": "36753fd7-8b2d-487b-82ae-dc318eca3ca6"
   },
   "source": [
    "We used generative AI tools such as ChatGPT and Claude to assist with technical aspects like using Python packages, improving code readability, and debugging issues. We also used them for creating visualizations and making the code more efficient. For example, helping us to find the right NumPy operations to optimize performance. The core algorithm design, theoretical understanding, data processing, and full implementation were based on material taught in class and additional study from online sources."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
